# Градиентный спуск и матричное дифференцирование

__Материалы с семинара:__

* [Презентация про градиентные спуски](https://github.com/FUlyankin/neural_nets_econ/blob/master/sem03_matrix_diff/nn_slides_2.pdf)
* [Задачки по градиентным спускам](https://github.com/FUlyankin/neural_nets_econ/blob/master/sem03_matrix_diff/tasks_02_grad.pdf)
* [Задачки про матричные производные](https://github.com/FUlyankin/neural_nets_econ/blob/master/sem03_matrix_diff/tasks_03_matrix_diff.pdf)
* Прошлогоднее решение [матричной части](https://github.com/FUlyankin/neural_nets_econ/blob/master/sem03_matrix_diff/matrix_diff.pdf) Лучше всего смотреть свой конспект. На семинарах мы записали всё лучше, чем в моём конспекте.

__Что можно поделать:__

* Дорешать ручные задачи из листочков. Всё, что не сможете решить, в следующий раз разберём.
* [Написать свою реализацию 50 оттенков градиентного спуска для логистической регрессии](https://github.com/FUlyankin/neural_nets_econ/blob/master/sem03_matrix_diff/HW2_gradient.ipynb)


__Ещё материалы:__

* [Методичка, по которой частично готовился](http://www.machinelearning.ru/wiki/images/5/50/MOMO17_Seminar2.pdf)
* [Конспект Жени Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/seminars/sem02-linregr-part1.pdf) про матричное диффириенцирование, [в репозитории](https://github.com/esokolov/ml-course-msu) много других конспектов
* [Задачник по ML Демешева,](https://github.com/bdemeshev/mlearn_pro/blob/master/mlearn_pro.pdf) часть задач на диффириенцирование из него
* [Матричный Coock book](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) с производными и вообще
* [Калькулятор матричных производных](http://www.matrixcalculus.org/)
* [Галерея функций потерь](https://losslandscape.com/)
