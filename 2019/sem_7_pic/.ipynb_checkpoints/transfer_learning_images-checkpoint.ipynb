{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "\n",
    "Каждый раз, обучая нейронку, мы сначала рандомно инициализируем веса, а после в ходе бэкпропа обучаем модель. Если мы сразу же угадываем хорошие веса, модель сходится быстрее. Иногда можно брать в качестве инициализации веса, полученные другими исследователями и на их основе дообучать модель под свой выход. Это здорово упрощает задачу обучения и экономит недели работы.\n",
    "\n",
    "Transfer learning это когда ты берёшь чужую модель и адаптируешь её под свою задачу. В этой тетрадке мы  посмотрим на то, как в tensorflow можно этим заняться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Реквизируем MobileNet \n",
    "\n",
    "В прошлый раз мы обсуждали то, как люди Imagenet рвали. В ходе этого обсуждения мы посмотрели на многие модные архитектуры. В самом конце мы сказали, что добившись высокого качества, Google решил, что надо бы сделать свои сетки более компактными, чтобы они влазили в мобилу. Одним из результатов этой работы стало создание [архитектуры MobileNetV2.](https://arxiv.org/abs/1801.04381) Она была собрана и обучена на ImageNet в апреле 2018 года. Если говорить грубо, MobileNet это почти как Inception, но меньше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow.keras.applications as zoo  # как зоопарк подгружаем, азазазазаз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте возьмём предобученную модель из Keras и просто попробуем её для чего-нибудь поиспользовать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = zoo.MobileNetV2(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опция `weights` отвечает за то, на каком датасете предобучена модель. Опция `include_top` отвечает за то, скачиваем мы модель полностью или только `feature extractor`, то есть только первые слои. Мы скачали всё. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Узнаем размерность входа и выхода\n",
    "input_shape = model.layers[0].output_shape[0][1:3]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем что-нибудь спрогнозировать. В модуле `keras.utis` есть функция `get_file`, которая умеет скачивать и разархивировать разные файлы. Будем использовать её для скачки картинок по ссылкам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "href = 'https://sadanduseless.b-cdn.net/wp-content/uploads/2019/06/cat-breading4.jpg'\n",
    "image = get_file('cat.jpg', href)\n",
    "\n",
    "print(image) # куда скачался то?! \n",
    "\n",
    "image = PIL.Image.open(image).resize(input_shape)\n",
    "image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# немного предобработки\n",
    "image = np.array(image)/255.0\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вот так можно добавить фиктивную размерность\n",
    "image[np.newaxis, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим прогноз \n",
    "result = model.predict(image[np.newaxis, ...])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! У нас есть $1000$ вариантов прогноза. Если быть более конкретным, нас в случае данного изображения устраивает класс номер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(result[0], axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].argsort()[-5:][::-1] # топ-5 классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только выяснить что это за класс. Для этого нам нужны метки Imagenet. Скачаем их. Они тоже уже есть в пакете. Удобно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import decode_predictions\n",
    "decode_predictions(result)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для строительства прогнозов и попробуем ещё. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_image(href, save_name):\n",
    "    img = get_file(save_name, href)\n",
    "    return img\n",
    "    \n",
    "def predict(image_path):\n",
    "    # Подгружаем изображение и делаем его ресайз в соответсвии с моделью\n",
    "    img = PIL.Image.open(image_path)\n",
    "    \n",
    "    # тут чуть подрбнее про ресайзы: https://habr.com/ru/post/247219/\n",
    "    img_resized = img.resize(input_shape, PIL.Image.LANCZOS)\n",
    "\n",
    "    # Рисуем картинку\n",
    "    plt.imshow(img_resized)\n",
    "    plt.show()\n",
    "\n",
    "    # Кнвертируем картинку в numpy и делаем лишнюю размерность\n",
    "    img_array = np.array(img_resized)[np.newaxis, ...]\n",
    "\n",
    "    # VGG-16 строит нам прогноз\n",
    "    pred = model.predict(img_array)\n",
    "    \n",
    "    # Декодируем прогноз\n",
    "    pred_decoded = decode_predictions(pred)[0]\n",
    "\n",
    "    # Печатаем его на экран\n",
    "    for code, name, score in pred_decoded:\n",
    "        print(\"{0:>6.2%} : {1}\".format(score, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = 'https://images-na.ssl-images-amazon.com/images/I/91NKh-FPcBL._SL1500_.jpg'\n",
    "path = save_image(href, 'shower_curtain.jpg')\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = 'https://sun9-34.userapi.com/c850216/v850216669/110118/s1XSv_XLgtY.jpg'\n",
    "path = save_image(href, 'the_most_awesome_boy.jpg')\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Последняя картинка как-то исказилась.__  Когда мы делаем `.resize`, мы используем опцию `PIL.Image.LANCZOS` она делает кое-какие приятные ништяки, связанные с защитой изображения от искажений. Вообще борьба с искажениями и разными размерностями у картинок - один из этапов предобработки. \n",
    "\n",
    "Как добиться того, чтобы картинка не искажалась? Использовать какие-то похожие фильры, либо обрезать картинки. Давайте попробуем написать в numpy функцию, которая будет заниматься обрезанием картинок вот по такой схеме: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/week3/images/center_crop.jpg\" style=\"width:50%\">\n",
    "\n",
    "Попробуйте самостоятельно сделать её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обрезания картинок \n",
    "def image_center_crop(img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    if w > h :\n",
    "        cropped_img = img[(w-h)//2:w-(w-h)//2,:,:]\n",
    "    elif w < h:\n",
    "        cropped_img = img[:,(h-w)//2:h-(h-w)//2,:]\n",
    "    else:\n",
    "        cropped_img = img\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставим эту функцию в построение прогнозов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    # Подгружаем изображение и делаем его ресайз в соответсвии с моделью\n",
    "    img = PIL.Image.open(image_path)\n",
    "    \n",
    "    # переделали в массив, обрезали \n",
    "    img = image_center_crop(np.array(img)) \n",
    "    \n",
    "    # вернули назад в картинку, сделали resize \n",
    "    img_resized = PIL.Image.fromarray(img).resize(input_shape)\n",
    "\n",
    "    # Рисуем картинку\n",
    "    plt.imshow(img_resized)\n",
    "    plt.show()\n",
    "\n",
    "    # Кнвертируем картинку в numpy и делаем лишнюю размерность\n",
    "    img_array = np.array(img_resized)[np.newaxis, ...]\n",
    "\n",
    "    # VGG-16 строит нам прогноз\n",
    "    pred = model.predict(img_array)\n",
    "    \n",
    "    # Декодируем прогноз\n",
    "    pred_decoded = decode_predictions(pred)[0]\n",
    "\n",
    "    # Печатаем его на экран\n",
    "    for code, name, score in pred_decoded:\n",
    "        print(\"{0:>6.2%} : {1}\".format(score, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = 'https://sun9-34.userapi.com/c850216/v850216669/110118/s1XSv_XLgtY.jpg'\n",
    "path = save_image(href, 'the_most_awesome_boy.jpg')\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `ImageNet` нет людей. Поэтому можно попробовать выяснить на кого именно ты сильнее всего похож. Чаще всего это будет либо подушка либо занавеска. Если выпадает что-то другое, тебе крупно повезло. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Твой код с картинкой\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transfer learning \n",
    "\n",
    "Побаловались с прогнозами, теперь давайте займёмся более серьёзными проблемами. Например, попробуем решить проблему тысячелетия вслед за [лучшими китайскими учёными.](https://www.youtube.com/watch?v=vIci3C4JkL0)\n",
    "\n",
    "\n",
    "![](https://www.semantics3.com/blog/content/images/downloaded_images/hot-dog-and-a-not-hot-dog-the-distinction-matters-code-included-8550067fb16/1-VrpXE1hE4rO1roK0laOd7g.png)\n",
    "\n",
    "\n",
    "По мотивам этой великой проблемы даже сделали [соревнование на kaggle.](https://www.kaggle.com/c/hotdogornot)  Но да лдано. Давайте лучше скачаем [папку с картинками](https://yadi.sk/d/AJra6kvvWV77nA) и приступим к работе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Данные \n",
    "\n",
    "Для начала попробуем применить нашу модель и посмотреть что будет происходить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нет, наш курс не мелкий! Просто я не придумал более хорошего места для данных на своём диске\n",
    "data_root = \"/Users/fulyankin/Yandex.Disk.localized/Выступления и мелкие курсы/NN_ranepa/hotdogornot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Забивать оперативную память лишними гигабайтами картинок не очень хочется, поэтому мы будем считывать картинки с жёсткого диска. Напишем для этого генератор, который будет считывать картинки по одной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# идём в керасовский модуль и берём из него функцию для генератора\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# внутри генератора есть очень широкие возможности для аугментации, на них вы посмотрите в дз\n",
    "image_generator = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# натравливаем гинератор на конкретную директорию c данными\n",
    "image_data = image_generator.flow_from_directory(data_root, \n",
    "                                                 batch_size=32,           # размер батча\n",
    "                                                 target_size=input_shape  # размер входных данных для сетки\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на характеристики батчей, которые лежат внутри генератора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in image_data:\n",
    "    print(\"Image batch shape: \", image_batch.shape)\n",
    "    print(\"Labe batch shape: \", label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на число шагов, которое собрался делать генератор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А ещё на названия классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data.class_indices.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте прогоним текущий батч через нейронку и посмотрим на прогнозы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим прогнозы по батчу из картинок\n",
    "result_batch = model.predict(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# што там у нас спрогнозировалось? \n",
    "labels_batch = [item[0] for item in decode_predictions(result_batch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на  картинки и построенные по ним прогнозы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 5\n",
    "rows = 3\n",
    "fig = plt.figure(figsize=(4 * cols - 1, 5 * rows - 1))\n",
    "\n",
    "k = 0 \n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(image_batch[k])\n",
    "        ax.set_title(\"{0:>6.2%} : {1}\".format(labels_batch[k][2], labels_batch[k][1]), size=14)\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель отрабатывает на уровне выше всех похвал (но это неточно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Дообучаем сетку\n",
    "\n",
    "Предобученная сетка не приспособлена для работы с нашими классами. Давайте заставим её их выучить. Для этого нам придётся срезать с сетки её последние слои. Посмотрим на `summary` модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно большое полотно. Давайте срежем в нашей сетки всё до слоя с логитом. Обратимся к слою, который мы хотели бы получить по имени. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = model.get_layer('global_average_pooling2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выход этого слоя мы будем использовать для создания своей нейронки. Этот слой - обычный тензор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Keras довольно просто приказать нашему графу выбрасывать на выход из модели нужный нам кусок. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "# Просто-напросто оформим нужный нам кусок нейросетки как модельку \n",
    "# Можно было бы оформить её как отдельный слой (сделаем это ниже)\n",
    "feature_extractor = Model(inputs=model.input,\n",
    "                   outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь построим на основе экстрактора новую модель. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "# Старт новой модели\n",
    "new_model = Sequential()\n",
    "\n",
    "# Первый слой в ней это экстрактор фичей \n",
    "new_model.add(feature_extractor)\n",
    "\n",
    "# можно добавить много промежуточных слоёв, если хочется\n",
    "\n",
    "# финальный софтмакс слой \n",
    "new_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, какие слои собрались обучаться. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_layer_trainable():\n",
    "    for layer in feature_extractor.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n",
    "        \n",
    "print_layer_trainable()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запретим Keras обучать веса внутри нашего экстрактора фичей. Это непозволительная роскошь. Тем более он уже предобучен на `Imagenet.` Запретим ему заниматься этим.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False\n",
    "\n",
    "for layer in feature_extractor.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print_layer_trainable( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем запускается ли модель, которую мы собрали и что она выдаёт на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_model.predict(image_batch)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наконец мы можем скомпилировать модель.  Оптимизировать всё это чудо будет нами горячо любимый Adam.  Каждую эпоху (полный проход по данным) в ходе алгоритма обратного распространения ошибки, мы будем обучать только последние слои. Одна эпохо в данном случае - проход по всем картинкам из наших папочек. Будем учить сетку $5$ эпох. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['acc']\n",
    ")\n",
    "\n",
    "# число шагов до конца первой эпохи\n",
    "steps_per_epoch = np.ceil(image_data.samples/image_data.batch_size)\n",
    "\n",
    "# учим сетку одну эпоху\n",
    "new_model.fit(image_data, epochs=5, \n",
    "            steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на текущие прогнозы сетки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим прогнозы по батчу из картинок\n",
    "result_batch = new_model.predict(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(image_data.class_indices.keys())\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(result_batch, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch = [classes[i] for i in np.argmax(result_batch, axis=1)]\n",
    "labels_batch[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch = list(zip(np.max(result_batch, axis=1), labels_batch))\n",
    "labels_batch[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 5\n",
    "rows = 3\n",
    "fig = plt.figure(figsize=(4 * cols - 1, 5 * rows - 1))\n",
    "\n",
    "k = 0 \n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(image_batch[k])\n",
    "        ax.set_title(\"{0:>6.2%} : {1}\".format(labels_batch[k][0], labels_batch[k][1]), size=14)\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно стало лучше :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tensor hub \n",
    "\n",
    "В $2017$ году Google сделал специальную платформу __TensorFlow Hub.__  Она используется для того, чтобы делиться друг с другом моделями. В таком формате свои результаты сохраняют различные исслодовательские группы. В том числе Россияская группа IPavlov. В следующие разы мы будем воровать у них модели через эту библиотечку.  \n",
    "\n",
    "Сейчас давайте посмотрим как это делается. Для начала нужно установить `tensorflow_hub` на свой компьютер через __pip.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(tf.__version__)\n",
    "print(hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Собираем модель\n",
    "\n",
    "По [ссылке](https://tfhub.dev) можно найти зоопарк моделей от Google для открытого использования через библиотеку. Мы выберем среди всего этого огромного числа моделек архитектуру [MobileNetV2,](https://arxiv.org/abs/1801.04381) обученную Google в апреле 2018 года на ImageNet. Эта сетка - развитие Inception. Подробнее про неё можно почитать либо в [оригинальной статье,](https://arxiv.org/abs/1801.04381) либо [на Хабре.](https://habr.com/ru/post/352804/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сетка подгружается библиотекое tensorflow_hub просто по ссылке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\" \n",
    "hub_module =  hub.load(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Дальше мы можем завернуть её в удобную функцию и начать применять. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [224, 224]\n",
    "\n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(url, input_shape=IMAGE_SHAPE+[3])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape такой, потому что три канала\n",
    "IMAGE_SHAPE + [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим сетку на изображении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = 'https://sadanduseless.b-cdn.net/wp-content/uploads/2019/06/cat-breading4.jpg'\n",
    "image = get_file('cat.jpg', href)\n",
    "\n",
    "print(image) # куда скачался то?! \n",
    "\n",
    "image = PIL.Image.open(image).resize(input_shape)\n",
    "image = np.array(image)/255\n",
    "\n",
    "classifier.predict(image[np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При желании можно узнать что за класс получился на выходе по аналогии с тем, как мы сделали это в самом начале тетрадки.\n",
    "\n",
    "## 3.2 Дообучаем сетку\n",
    "\n",
    "Доучивать её мы будем на тех же самых данных по хотдогам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in image_data:\n",
    "    print(\"Image batch shape: \", image_batch.shape)\n",
    "    print(\"Label batch shape: \", label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам для дообучения понадобится только feature extractor, его мы и скачаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что будет идти из этого экстрактора на выход. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запретим tensorflow тренировать веса экстрактора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cобираем модель, на последний слой вытаскиваем softmax на 7 классов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_extractor_layer,\n",
    "  Dense(image_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем запускается ли модель, которую мы собрали и что она выдаёт на выходе\n",
    "result = model.predict(image_batch)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем модель и запускаем обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['acc']\n",
    ")\n",
    "\n",
    "# число шагов до конца первой эпохи\n",
    "steps_per_epoch = np.ceil(image_data.samples/image_data.batch_size)\n",
    "\n",
    "# учим сетку одну эпоху\n",
    "model.fit(image_data, epochs=3, \n",
    "            steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с кодом выше можно посмотреть что наша модель будет прогнозировать. Не будем копипастить. Лучше сделаем другую вещь. Мы довольно долго учили модельку. Давайте её сохраним на компьютер. Модели на комп можно сохранять в разных видах и форматах. Про это ещё поговорим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "export_path = tf.keras.models.save_model(model, \"./saved_models\")\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls saved_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы здесь не дробили выборку на трэйн и тест, а просто дообучали сетку сразу и на всём. Делали мы это для того, чтобы тетрадка выглядела попроще. В домашке вы заведёте себе два генератора. Один для трэйна, второй для теста. И побегут эпохи для обучения сеток.\n",
    "\n",
    "\n",
    "## Почиташки \n",
    "\n",
    "Делал этут тетрадку на основе двух туториалов: \n",
    "\n",
    "* [Первый,](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb)  на этом гитхабе довольно большое количество крутых тетрадок про нейронки. Рекомендую позалипать на них. \n",
    "* [Документация Tensorflow_Hub](https://www.tensorflow.org/hub) и [пример](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) её применения на цветочках\n",
    "* [Пример извлечения фичей для SWM из нейронки,](https://www.kaggle.com/craigglastonbury/using-inceptionv3-features-svm-classifier) если вдруг это кого-то заинтересовало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
